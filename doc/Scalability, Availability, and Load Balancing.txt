Scalability and Elasticity
Scalability 
describes how well a component performs when presented with increased load. There are two (common) varieties - vertical & horizontal.
Vertical Scaling 
(Scale 
up
) - Upgrade existing hardware
No new resources are added.
Existing resources made more efficient through upgrade of hardware - CPU, RAM, etc.
Pros
Easier to manage - doesn’t require modifying load balancers, code changes (typically), etc.
Increases component's ability to handle additional load when under stress.
Cons
Increase in cost
Further scale is constrained to limitations of hardware
Without horizontal scaling - still maintains component as single point of failure.
Horizontal Scaling 
(Scale 
out
) - Add more hardware
More physical resources are added - servers, disks, etc.
Pros
Helps manage traffic more effectively - can handle larger number of requests.
Increased performance due to load distribution over many machines.
Increased fault tolerance with failover.
Cons
Increase in cost - typically more cost-effective than vertical scaling.
Increase in complexity - introduces need to manage more server nodes & load balance them, appropriately.
Elasticity
 describes the ability of cloud resources to automatically scale up, down, out, or in so that increases or decreases in workload volume can be handled effectively.
Availability
Availability 
describes the uptime of a system and is typically conveyed through a measure of 9’s. For example, if a system is said to have 99.9% availability (or 
three 9’s
) that means that the acceptable 
total
 downtime for an entire year would be approximately 
8 hours, 46 minutes
. Four 9’s (99.99%) reduces that value to approximately 
52 minutes, 36 seconds
. That’s a big difference!
Availability Strategies
Failover
 - when one component of a system fails, it can be replaced by another. There are two varieties
Active-Passive 
- A heartbeat is sent between the active and passive server on standby. If the acceptable threshold for response is not met, the passive server will take over the active server’s roles and service will continue.
Active-Active
 - Both servers are up and running at the same time - traffic is split between them. If one server goes down, all traffic goes to the other server and vice-versa.
Replication
 - Data is written to a primary server & then replicated across other nodes in a cluster
Active-Active
 - Data is both written to and read from both active replicas. If one replica goes down, the other replica can take its place.
Active-Passive 
- Data is written to (and sometimes read from) a primary server, then replicated to a passive server. If the active server goes down, the passive server can take its place.
Note that Replication and the consequences thereof are a massive topic on their own. There is a plurality of strategies for distributing load between databases - sharding and federation are two examples. There are also many varieties of replication for databases - single-leader, multi-leader, leaderless, chain, etc.
Load Balancing
With scalability and availability out of the way, 
Load Balancing 
is a strategy to accomplish both. Load balancers are system components that distribute incoming traffic across a backend pool of resources - they can be either hardware or software components. There are two common varieties of load-balancing, corresponding to the 
OSI networking model
.
Additionally, Load Balancers are just components like anything else and they also can utilize the same strategies presented above for failover.
Layer 4
 - Transport Layer
Layer 4 load balancing occurs just after the network layer, once the request has been sent. Typically, L4 load balancing will involve evaluation of the source and destination IPs to determine how to route traffic appropriately.
e.g. Customers on the West Coast will be routed to servers hosted closest to their physical location. Likewise, customers on the East Coast will be routed to servers hosted closest their own physical location.
Layer 7 
- Application Layer
Layer 7 load balancing occurs at the topmost layer of the OSI model and will typically evaluate things like HTTP headers, cookies, or message body of a request. Layer 7 load balancing is typically used to keep traffic separated by responsibility or user identity.
e.g. In an active-active system - a user needs to read their own writes in a database for consistency. L7 load balancing allows the user to be uniquely identified & routed to the same database server each time.
e.g. A company hosts their public website with video & image content on one server and a payments system on another server - both servers are behind the same load balancer. The load balancer can evaluate incoming traffic and route users to the appropriate place based on their request.
 URL:/spaces/EA/pages/4070572611/Scalability+Availability+and+Load+Balancing