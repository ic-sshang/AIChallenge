Intro
This set of articles provides information on Kubernetes and the best practices for use of Kubernetes at InvoiceCloud.
Child Articles
true
Article Table of Contents
1
6
false
list
brackets
true
What is Kubernetes?
Kubernetes (or commonly written as K8s) is a 
container orchestrator
.
It has the ability to be extended and customized by using existing or creating new plug-ins at interface points.
It is designed and built with codification and automation in mind enabling consistency amongst environments and application replicas within an environment.
It has become a key part of most organization's 
Application Platform
 framework.
Quick History
Kubernetes was announced by Google in mid-2014
1
. 
The project was created by Joe Beda, Brendan Burns, and Craig McLuckie, who were soon joined by other Google engineers, including Brian Grant and Tim Hockin
1
. 
Kubernetes has its roots in Google’s internal Borg System, introduced between 2003 and 2004
2
. 
Later, in 2013, Google released another project known as Omega, a flexible, scalable scheduler for large compute clusters. In that same year, McLuckie, Beda, and Burns set out to develop a “minimally viable orchestrator”
2
.  Main goal, enable easy, efficient, effective, and automated management of large amounts of Compute and Working Memory in which many complex applications and services run.
What is a Container?
A container image is a 
lightweight, standalone, executable package of software
 that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.
What is an Application Platform?
We define an application platform as a viable place to run workloads.
The application platform is often where we find ourselves at the intersection of apps and infrastructure.
Typical Workload Architectures within Kubernetes
There are several types of common Workload Architectures within Kubernetes, we will outline the main 3 from an Application Workload Hosting perspective:
Fanout API or UI
CRON Job
KEDA Job
Fanout API or UI
The fanout API or UI workload architecture might be one of the most prevalent uses of Kubernetes as it provide prebuilt solutions for application load balancing, traffic routing, and TLS termination.
Below is a diagram of the standard fan out architecture:
1
0
3633119280
3061252114
true
common-k8s-workload.drawio.svg
2
0
6
https://invoicecloud.atlassian.net/wiki
common-k8s-workload.drawio.svg
0
w06maZc1cbX3MX71phJN 1
1439.5
auto
top
1
404.5
Ingress
The Ingress Controller allows clients outside of the K8s Cluster to access the workload hosted within the Pods of the Cluster.
In essence, the Ingress Controller provides a Domain Name to the workload hosted in the Cluster.
The Ingress Controller forwards the incoming traffic to the associated Service (Workload Load Balancer) based upon Routing Rules defined in the Ingress which are based on Incoming Hostname and Path.
Each Workload registers its Ingress Rules with the Ingress Controller using the 
Ingress
 Kubernetes Resource Type.
TLS is terminated at the Ingress Controller (but traffic can be encrypted as traffic is sent to the Service and then Pod).
Service
The Service typically acts as a Load Balancer within a K8s workload, routing requests to a highly available set of Pods, giving the set of Pods a single IP Address clients can use when connecting to the workload.
The requests are routed to Pods within the K8s Cluster based upon the labels applied to the Pods. The Service Resource Definition will contain a Pods Selector defining the Labels the Service will look for on Pods when routing traffic.
Services listen on a specific Port that is specified in their manifest.
Pod
The Pod is where the Containers live and run. They have a private IP Address themselves, but are mainly accessed via the Service's private IP Address.
Pods have labels assigned to them that are used by the Service to route traffic to the correct application.
A Pod can contain one or more Containers. Each Container can listen for traffic on a different Port.
The Pod, and the images that run in containers inside the Pod, are the most vulnerable Resources within the K8s Cluster. This is because vulnerabilities within the code of the workloads running in the Pod can provide attackers with penetration vectors that enable the attacker to access the Container's Operating System.
Exploits in Front End applications have been known to allow an attacker to access the Shell of the container running the Front End application. Hence the majority of the K8s Security Policies are related to enabling configurations that minimize the Blast Radius if a container is compromised.
Cluster
The K8s Cluster is a collection of Nodes (physical or virtual computers) that allow for instances of the Application Pods to be hosted on separate Nodes enabling high availability of the Application (workload). 
All Kubernetes workloads reside within the Cluster.
CRON Job
A CRON Job is very simple workload architecture in Kubernetes where a Pod is run on a time-based schedule.  Each invocation of the Pod run creates a temporary Pod with a unique Pod Name, the Pod lives long enough to execute whatever Job(s) run inside the Container(s) of the Pod.
KEDA Job
KEDA is a Kubernetes-based Event Driven Autoscaler. With KEDA, you can drive the scaling of Pods in Kubernetes based on the number of events needing to be processed by the workload hosted in the Pod, such as Messages in an Azure Service Bus.
Adding Resources to a K8s Cluster
Resources get added to a K8s Cluster using the K8s Cluster Control Plane API.  The Control Plane API is normally interacted with by the Operator via the 
kubectl
 command line tool and the specifics of the Resources are normally represented with a K8s Resource Manifest file.  The K8s Resource Manifest file is a 
.yaml
 based file that follows the structure of the API Model for a specific Resource Type.
An example of one flavor of this interaction is shown below:
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: test-namespace
spec:
  serviceAccountName: ${SERVICE_ACCOUNT_NAME}
  containers:
    - image: my-container-image:latest
      name: test-container
      env:
      - name: ENV_VARIABLE
        value: value
  nodeSelector:
    kubernetes.io/os: linux
EOF
Helm
Helm is an extension of the K8s Manifest pattern which allows multiple K8s Resource Manifests to be bundled together and codified enabling consistent deployment complex K8s workload topologies across environments.
Helm charts that InvoiceCloud has developed:  
helm-charts - Repos (azure.com)
 
Current InvoiceCloud Usages of Kubernetes
Payer NextGen
The Payer NextGen was a fully modern project that implemented a React based SPA UI integrated with our current Payer Portal and interacted with business logic and the database via RESTful APIs designed and deployed in a Microservice architecture.  This workload and infrastructure has been deployed all the way thru to our Production Environment.
The infrastructure and foundational Kubernetes configuration are managed in the following Repo:
Payer_Infrastructure - Repos (azure.com)
APIs and UIs Hosted
One Time Payment Service
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/OneTimePaymentService
Payer Pay By Text Service
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/PayerPayByTextService
Payer Pay By Text UI (Not Included in the Production Deployment yet)
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/PayerPayByTextUI
Payer Portal UI
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/PayerPortal
Payer Account Service
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/PayerAccountService
Payer Branding Service
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/PayerBrandingService
Payer User Service
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/PayerUserService
Payer Support Service
Repo: 
https://dev.azure.com/invoicecloud/Src/_build?definitionId=2230
Payer Support UI
Repo: 
https://dev.azure.com/invoicecloud/Src/_git/PayerSupportUI
Deployed with Helm chart: 
web - Repos (azure.com)
BillerMailgunStatusJob
A modern CRON Job based background service that collects data from Mailgun APIs concerning the delivery status of queued Email.  This feature is available for Billers to opt into.  This workload and infrastructure has been deployed all the way thru to our Production Environment.
Currently is deployed to the set of environmental Payer Clusters but will soon be transitioned to the 
planned set of environmental Biller Clusters
 to enable domain segmentation in our infrastructure architecture.
Application Repo: 
BillerMailgunMessageStatusJob - Repos (azure.com)
Deployed with Helm chart: 
cronjob - Repos (azure.com)
Kong API Gateway
Kong is licensed software we have purchased and deployed to the Kong K8s Cluster that serves as an API Router that has the ability to mutate and secure API URLs and Payloads.  Kong also has the ability to act as a Developer Portal enabling the internal and external discovery and consumption of API Services.  This workload and infrastructure has been deployed all the way thru to our Production Environment.
The infrastructure and application configuration are managed in the following Repo:
Kong_Infrastructure - Repos (azure.com)
Deployed with Helm chart: 
charts/charts/kong/values.yaml at main · Kong/charts (github.com)
Azure DevOps Agents
We have NonProd and Prod AzDo agent pools that are hosted in Kubernetes.
The infrastructure and foundational Kubernetes configuration are managed in the following Repo:
Tooling_Infrastructure - Repos (azure.com)
Container Image: 
Dockerfile - Repos (azure.com)
Containerization of Proxy Services and Web Services
In 2023 we ran a project to containerize the InvoiceCloud Proxy Services.  We were able to successfully containerize the Database, Business, and RTDR Services and prove they worked as expected by creating a Proxy Service Integration test suite for each of them.  These were hosted in the 
Proxy K8s Cluster
 we had deployed at the time. 
Along with the containerization of the Proxy Services, we also began containerizing the Web Services starting with the ICDocumentation and ICCRM Web Apps.  We were able to successfully containerize both Web Apps.   
However, the project was put on hold but is ready to be restarted when the time is right.
More info can be found:
K8s Migration Team - Confluence (atlassian.net)
Proxy Spoke - Confluence (atlassian.net)
Web Spoke - Confluence (atlassian.net)
https://dev.azure.com/invoicecloud/Src/_git/ICCRM?version=GBK8_Integration&path=/Dockerfile
https://dev.azure.com/invoicecloud/Src/_git/ICDocumentation?path=/Dockerfile
https://dev.azure.com/invoicecloud/Src/_git/ICDatabaseService?path=/Dockerfile
https://dev.azure.com/invoicecloud/Src/_git/ICBusinessService?path=/Dockerfile
https://dev.azure.com/invoicecloud/Src/_git/ICRTDRService?path=/Dockerfile&version=GBk8-migration&_a=contents
Benefits of Containerization and Kubernetes
As mentioned previously, Google had a complex problem to solve, how do you manage large amounts of complex applications and service that process large amounts of data in parallel and do it at scale consistently and effectively.  The nature of this problem required large amounts of CPUs and Working Memory.  Their solution was Containers and Container Orchestration, which evolved into the Kubernetes Platform.  Luckily for the industry, Google open-sourced the technology.  Many organizations have realized the following benefits:
Application, Resource, and Process isolation
Applications can evolve their needs and dependencies independently and not affect other applications running on the same set of CPU and Memory.
Resource Requests and Limits can be applied to each Application hosted in a K8s Cluster avoiding a rogue process in a single Application affecting all applications deployed to a VM.
Automated Resiliency and Recovery
The goal of Replica Sets and Node Pools is High Availability, so if a Node (VM in a VM Scaleset) goes bad or encounters a disaster, the K8s Cluster will recover and automatically replace the Node.  If a Pod in a Replica Set goes bad, K8s will automatically replace the Pod.
K8s has built-in functionality to allow for 
Pod/Application Probing
, which ensures Application availability and automates the recovery from rogue processes within an Application Replica.  
Efficient usage of CPU and Memory with Automated Scaling
Applications can specify Resource Requests for each Replica that the K8s Engine can use to efficiently balance Application Replicas for many different Applications amongst the Nodes (VMs) managed by the K8s Cluster.
Consistency and Parity in Environments at InvoiceCloud
Using the new Kubernetes Cluster or requesting a Cluster for use for your team enables you to take advantage of the Benefits from Infrastructure as Code here at InvoiceCloud which yields full parity between Environments from an Infrastructure, Networking, and Configuration perspective (note scale of resources will be lower in lower Environments).  This enables the identification of issues with application code and workload configuration earlier in the SDLC (shift left).   
Related Articles
Next Gen Architecture - Engineering Documentation - Confluence (atlassian.net)
Windows .Net Framework 4.8+ Application Containerization - NET - Engineering Documentation - Confluence (atlassian.net)
Proxy Spoke - Confluence (atlassian.net)
Docker - Platform - Confluence (atlassian.net)
 URL:/spaces/platform/pages/3061252114/Kubernetes