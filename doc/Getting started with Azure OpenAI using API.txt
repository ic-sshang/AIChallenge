This document is intended to provide enough basic information to get started using our private instance of Azure OpenAI.  With the basics in hand, they can be used to apply to other access methods like libraries and frameworks.
1
6
false
default
list
true
Required Values
Values
These value pairs are required for our REST API call and other access methods
Name
Value
Notes
Deployment Name
cd-pri-playground2-dev
The Deployment Name is directly tied to the LLM used.  This Deployment uses 
gpt-4o-mini
API Base Url
https://ca-pri-playground2-dev.openai.azure.com
API Key
Retrieve from Azure Keyvault instance 
kv-glb-vault1-dev
 (
link
).  Key name is 
Playground-OpenAi-ApiKey
If you don’t have access to this keyvault as an engineer, please seek out Platform team for access.
API Version
2025-01-01-preview
Endpoint
POST <API Base Url>/openai/deployments/<Deployment Name>/chat/completions?<API Version>
Headers
The API Key value is passed in through the 
api-key
 header.
Header
Value
Notes
api-key
<API Key>
Body/Payload
The payload below asks the LLM a basic 
What is 5*5
 question.  Modify as needed for your use case.  
js
wide
760
{
    "messages": [
        {"role": "system", "content": "You are an assistant."},
        {"role": "user", "content": "What is 5*5?"}
    ]
}
Calling API
Use your favorite API development tool to wrap up the call to the API.  Below are some code snippets that are applicable within the InvoiceCloud ecosystem that are good references.  
HTTP
wide
760
POST /openai/deployments/cd-pri-playground2-dev/chat/completions?api-version=2025-01-01-preview HTTP/1.1
Host: ca-pri-playground2-dev.openai.azure.com
api-key: <API Key>
Content-Type: application/json
Content-Length: 148

{
    "messages": [
        {"role": "system", "content": "You are an assistant."},
        {"role": "user", "content": "What is 5*5?"}
    ]
}
C#
Using HttpClient:
c#
wide
760
var client = new HttpClient();
var request = new HttpRequestMessage(HttpMethod.Post, "https://ca-pri-playground2-dev.openai.azure.com/openai/deployments/cd-pri-playground2-dev/chat/completions?api-version=2025-01-01-preview");
request.Headers.Add("api-key", "<API Key>");
var content = new StringContent("{\r\n    \"messages\": [\r\n        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"What is 5*5?\"}\r\n    ]\r\n}", null, "application/json");
request.Content = content;
var response = await client.SendAsync(request);
response.EnsureSuccessStatusCode();
Console.WriteLine(await response.Content.ReadAsStringAsync());
Using RestSharp:
c#
wide
760
var options = new RestClientOptions("https://ca-pri-playground2-dev.openai.azure.com")
{
  MaxTimeout = -1,
};
var client = new RestClient(options);
var request = new RestRequest("/openai/deployments/cd-pri-playground2-dev/chat/completions?api-version=2025-01-01-preview", Method.Post);
request.AddHeader("api-key", "API Key");
request.AddHeader("Content-Type", "application/json");
var body = @"{" + "\n" +
@"    ""messages"": [" + "\n" +
@"        {""role"": ""system"", ""content"": ""You are an assistant.""}," + "\n" +
@"        {""role"": ""user"", ""content"": ""What is 5*5?""}" + "\n" +
@"    ]" + "\n" +
@"}";
request.AddStringBody(body, DataFormat.Json);
RestResponse response = await client.ExecuteAsync(request);
Console.WriteLine(response.Content);
Python
Using 
requests
py
wide
760
import requests
import json

url = "https://ca-pri-playground2-dev.openai.azure.com/openai/deployments/cd-pri-playground2-dev/chat/completions?api-version=2025-01-01-preview"

payload = json.dumps({
  "messages": [
    {
      "role": "system",
      "content": "You are an assistant."
    },
    {
      "role": "user",
      "content": "What is 5*5?"
    }
  ]
})
headers = {
  'api-key': '<API Key>',
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)

The Response
Below is the response using the above request details.  Some key values that could be pertinent to your use case are:
message
This is the response from the LLM to your prompt
model
This gives you a very good indicator of the LLM it is using.  
completion_tokens
prompt_tokens
total_tokens
LLMs and how they are configured determine how many “tokens” they can work with.  As your prompts and answers get longer you should be conscious of the token limitations on the LLM.  
json
wide
760
{
    "choices": [
        {
            "content_filter_results": {
                "hate": {
                    "filtered": false,
                    "severity": "safe"
                },
                "protected_material_code": {
                    "filtered": false,
                    "detected": false
                },
                "protected_material_text": {
                    "filtered": false,
                    "detected": false
                },
                "self_harm": {
                    "filtered": false,
                    "severity": "safe"
                },
                "sexual": {
                    "filtered": false,
                    "severity": "safe"
                },
                "violence": {
                    "filtered": false,
                    "severity": "safe"
                }
            },
            "finish_reason": "stop",
            "index": 0,
            "logprobs": null,
            "message": {
                "content": "5 multiplied by 5 equals 25.",
                "refusal": null,
                "role": "assistant"
            }
        }
    ],
    "created": 1745523111,
    "id": "chatcmpl-BPwaV5UwVWamt7FM8ZFnjmSjHclnT",
    "model": "gpt-4o-mini-2024-07-18",
    "object": "chat.completion",
    "prompt_filter_results": [
        {
            "prompt_index": 0,
            "content_filter_results": {
                "hate": {
                    "filtered": false,
                    "severity": "safe"
                },
                "jailbreak": {
                    "filtered": false,
                    "detected": false
                },
                "self_harm": {
                    "filtered": false,
                    "severity": "safe"
                },
                "sexual": {
                    "filtered": false,
                    "severity": "safe"
                },
                "violence": {
                    "filtered": false,
                    "severity": "safe"
                }
            }
        }
    ],
    "system_fingerprint": "fp_ded0d14823",
    "usage": {
        "completion_tokens": 10,
        "completion_tokens_details": {
            "accepted_prediction_tokens": 0,
            "audio_tokens": 0,
            "reasoning_tokens": 0,
            "rejected_prediction_tokens": 0
        },
        "prompt_tokens": 23,
        "prompt_tokens_details": {
            "audio_tokens": 0,
            "cached_tokens": 0
        },
        "total_tokens": 33
    }
}
Plug and Play Projects
Use these starter projects that already have all the details above plugged in.  You just need to plug in the API Key
Type
Download/Link
Note
 Postman Collection
 
Visual Studio / C# 8.0 Project
 
Helpful Links
Microsoft - 
Quickstart: Get started using chat completions with Azure OpenAI Service
 URL:/spaces/EA/pages/4248797192/Getting+started+with+Azure+OpenAI+using+API