1
6
false
default
list
true
Testing Strategy
Load testing round 1 against the NonProd Kong instance is scheduled for Friday 4/19/2024 using a 2 pronged approach:
Azure Load Testing
 the publicly exposed endpoints in `invoicecloud-beta.com`
K6 tests
 from AVD against the Kong Routes not yet exposed publicly
We monitored the Cluster Metrics and Kong Traffic using the following NewRelic Panes:
Cluster Summary
Cluster Metrics
Cluster Traffic
We also analyzed load test results from both test frameworks as well.
Kong Configuration
Node Size:  2 CPU and 8 RAM
Kong API Gateway Pods per Node: 1
Minimum Nodes/Pods:  3
Maximum Nodes/Pods:  6
Node Pod Resource Request Limit Configuration:
  Namespace                   Name                                      CPU Requests  CPU Limits   Memory Requests  Memory Limits  Age
  ---------                   ----                                      ------------  ----------   ---------------  -------------  ---
  kong                        api-gateway-kong-7598765985-cb8z5         700m (36%)    1200m (63%)  600Mi (11%)      3000Mi (56%)   133m
  kube-system                 azure-ip-masq-agent-m9xkh                 100m (5%)     500m (26%)   50Mi (0%)        250Mi (4%)     4d20h
  kube-system                 cloud-node-manager-j6lnx                  50m (2%)      0 (0%)       50Mi (0%)        512Mi (9%)     4d20h
  kube-system                 csi-azuredisk-node-z4ntr                  30m (1%)      0 (0%)       60Mi (1%)        400Mi (7%)     4d20h
  kube-system                 csi-azurefile-node-bmlhc                  30m (1%)      0 (0%)       60Mi (1%)        600Mi (11%)    4d9h
  kube-system                 kube-proxy-cqc7s                          100m (5%)     0 (0%)       0 (0%)           0 (0%)         4d20h
  kube-system                 metrics-server-5f476446c6-kbfsv           50m (2%)      145m (7%)    89Mi (1%)        359Mi (6%)     111m
  newrelic                    newrelic-bundle-newrelic-logging-lgr8t    150m (7%)     300m (15%)   64Mi (1%)        128Mi (2%)     4d20h
  newrelic                    newrelic-bundle-nrk8s-kubelet-w8sd2       200m (10%)    0 (0%)       300M (5%)        600M (10%)     4d20h
  
Allocated resources:
  Resource           Requests          Limits
  --------           --------          ------
  cpu                1410m (74%)       2145m (112%)
  memory             1320264448 (23%)  6103975424 (108%)
  ephemeral-storage  0 (0%)            0 (0%)
  hugepages-1Gi      0 (0%)            0 (0%)
  hugepages-2Mi      0 (0%)            0 (0%)
Kong HPA Configuration (note: As of Kubernetes 1.9 HPA calculates pod CPU utilization as total CPU usage of all containers in pod divided by total request):
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
Load Test Metrics
https://onenr.io/0PwJmp3ELQ7
https://onenr.io/0BQ1YA7Y5Qx
https://onenr.io/0BQ1YA7Y5Qx
https://onenr.io/0BQ1YA7klQx
https://onenr.io/0PwJmp3kpQ7
https://onenr.io/0dQegV2eNRe
https://onenr.io/07wkxvnGkRL
Load Test Summary
3 Spikes of 30K or more requests per minute
Final Spike of the 3 Spikes, which was largest at ~45k requests per minutes maxed the Node CPU utilization at ~35%
The Final Spike resulted in a 
HPA Scale Event
 of 1 additional Node/Pod Horizontally scaling out, which means 70% or more usage of at least one Podâ€™s CPU Request which would be 490 of 700 milli-cpus.  However, the total Node Utilization was relatively low so we should be able to up the Pod CPU Resource Request to [900m - 1000m] and Limits to [1500m - 1600m].
Memory Utilization was stable and never exceeded 42% Node Utilization.  However, the Average Kong Pod Memory Utilization was 650 MB, which exceeded our Request of 600MB, so that Request should be bumped to 700 MB.
Latency and Latency Spikes were due to slow responses from DevTest VMs that served the CRM Login screen.  Response times from Payer NextGen health checks and response times from the Kong UI and API were very low and well within acceptable limits, both of which are Kubernetes Cluster hosted.
Questions
This load testing incorporated only one , less sized VM than production. Should we plan for a brownout testing against production upstream web farm during maintaiance window to get better understanding of latency against procution?
Response
We used DevTest VMs 1, 2, 3, 4, 5, and 14
I think a Prod Load Test during a maintenance window would be fine.
Regardless of above, does next load testing plan include Node scaling and to measure times taken to scale up one more new node based on the load? 
I am suspecting, we may be incapable of generating load high enough to spin a new node off on devtest single upstreams?
Response
We did have an HPA scale event, an additional Node and Pod scaled up under the highest load as mentioned in the Load Test Summary item 3.  This is also shown in the graphs above.
 URL:/spaces/platform/pages/3193503746/Kong+DV2+-+Round+1+Load+Testing