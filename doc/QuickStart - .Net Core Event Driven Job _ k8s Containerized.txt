1. Create the Worker Service to Act as an Event Consumer
Create a new project in Visual Studio, either for an existing solution or a new one. When presented with the 
Create a new project
 menu, find the 
Worker Service
 template in the list of items and click 
Next
.
Visual Studio Worker Service Option
Continue along to the 
Configure your new project
 page - give the project a strongly named title indicating its domain and responsibility.
{Domain}.{Service/SubSystem}.Consumer
 or 
{Domain}.{Service/SubSystem}.Worker
e.g. 
Biller.ReportGenerator.Consumer
Visual Studio - Name project
On the Next page, select 
.NET 8.0 (Long Term Support)
 as the Framework.
Make sure 
Enable Container Support
 is checked
Set the Container OS to 
Linux
Set the Container build type to 
DockerFile
Make sure the 
Do not use top-level statements
 checkbox is checked.
This one isn’t a huge deal - but it will save you from manually having to add additional code once the project is created.
Visual Studio - Other project settings
2. Docker Setup
If you followed the guide above and created the project with container support enabled, you should have had a 
Dockerfile
 created alongside your project. 
If you have not created a 
Dockerfile
, you can run 
docker init
 from a terminal within the same location as your 
.csproj
 file to generate 
Dockerfile
, 
.dockerignore
, etc. However, do note that the output of 
docker init
 is quite different than what Visual Studio creates automatically - see the below section on re-aligning this with the Visual Studio-style 
Dockerfile
Example of newly scaffolded project structure
 1. Once a 
Dockerfile
 exists in the repository - open it and verify the contents appear similar to the example on the right.
(Optional)
 - In the section annotated with 
Addition Section 1
 in the example 
Dockerfile
 - add another 
COPY
 task for any additional project dependencies you have for the 
Consumer
 - this would include your 
.Contracts
 project, etc.
e.g. 
MyDomain.MyService.Contracts/MyDomain.MyService.Contracts.csproj
NOTE - 
You 
do
 need to indicate the relative path of resources from where the 
docker build
 command will be run.
e.g. 
../../Folder/PathToMyProject.csproj
In the section annotated with 
Addition Section 2
, near the bottom of the file, we need to add a Linux user that has permissions for the 
/app
 folder the worker will be running in - this is necessary to avoid running the container as 
root
 and resultantly introducing a security vulnerability.
Add the following content, specifying the user or group name as appropriate. 
docker
# Add a User Group & User to run the application. This is a best practice to avoid running as root in production.
RUN addgroup --group event-consumers --gid 2000 \
	&& useradd \
	--uid 1000 \
	--gid 2000 \
	"workersvc" 

# Grant ownership of the '/app' folder to the "workersvc" user in the "event-consumers" group.  
RUN chown workersvc:event-consumers /app

# Specify that the "workersvc" user should be used to run the application. This ensures the app does not run as root.
USER workersvc:event-consumers
The last step that is advisable here is to simply try running the 
Dockerfile
 and seeing if it builds correctly.
docker build -f path/to/project/Dockerfile .
If the Docker image builds successfully, then we’re done here - the app has been successfully containerized & is ready for Azure Container Registry.
docker
ARG NUGET_USERNAME
ARG NUGET_PAT
ARG SEMVER=0.0.0.0
ARG NUGET_VERSION=0.0.0

# This stage is used when running from VS in fast mode (Default for Debug configuration)
FROM mcr.microsoft.com/dotnet/runtime:8.0 AS base

WORKDIR /app

# This stage is used to build the service project
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
ARG NUGET_USERNAME
ARG NUGET_PAT
ARG SEMVER
ARG NUGET_VERSION
ENV NUGET_USERNAME=${NUGET_USERNAME}
ENV NUGET_PAT=${NUGET_PAT}
WORKDIR /src

# Example - This is assuming `.sln` is in the root of the directory, docker build is running from the same location, and all projects are located in `src/` folder.

# ***Addition Section 1***

# Copy any additional projects that the primary project depends on - Infrastructure, Common, Contracts, DAL, Data, Domain, etc.
# COPY ["src/MyDomain.MyService.Contracts/MyDomain.MyService.Contracts.csproj", "MyDomain.MyService.Contracts/"]

# Copy NuGet files over for 'dotnet restore' to work properly - credentials should be provided as ARG values.
# COPY ./Directory.Packages.props .
# COPY ./nuget.config . # See here for an example using ARGS - https://dev.azure.com/invoicecloud/Biller/_git/BillerReportingAPI?path=/Dockerfile

COPY ["src/MyDomain.MyService.Consumer/MyDomain.MyService.Consumer.csproj", "MyDomain.MyService.Consumer/"]

RUN dotnet restore -p:Configuration=Release-v:n "./MyDomain.MyService.Consumer/MyDomain.MyService.Consumer.csproj" || exit 1
COPY . .
WORKDIR "/src/MyDomain.MyService.Consumer"
RUN dotnet build "./MyDomain.MyService.Consumer.csproj" -c:Release -p:Version=$SEMVER -p:NuGetVersion=$NUGET_VERSION -o /app/build

# This stage is used to publish the service project to be copied to the final stage
FROM build AS publish
ARG BUILD_CONFIGURATION=Release
RUN dotnet publish "./MyDomain.MyService.Consumer.csproj" -c $BUILD_CONFIGURATION -o /app/publish 

# This stage is used in production or when running from VS in regular mode (Default when not using the Debug configuration)
FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .

# ***Addition Section 2***


ENTRYPOINT ["dotnet", "MyDomain.MyService.Consumer.dll"]
3. Configure Rebus
Prerequisites
Review Best Practices on Message - Commands + Events
 
Review Service Bus Messaging Concepts
OPTIONAL 
- 
Refer to the Rebus documentation
. Bookmark it and keep it nearby
A. Add Necessary Packages
We will only need a handful of NuGet packages for Rebus- one serves as the core framework and the other is specific to the transport we are using (Azure Service Bus).
Install the following into your solution
powershell
Install-Package Rebus
Install-Package Rebus.AzureServiceBus

# For Applications using the `IServiceProvider` for Dependency Injection.
Install-Package Rebus.ServiceProvider
B. Create the 
Contracts
 Project
One of the most important benefits of having an event-driven system is the decoupling of producers/publishers and consumers/subscribers. To do this, we expose our commands & events as “Contracts” that communicate the API of our service to external parties.
The 
Contracts
 project can be pretty lightweight, and it should only contain your 
Commands
, 
Events
, and any supporting value objects (like 
enum
 or 
record
 types) that are required by either.  It may also contain relevant settings or config classes that external parties may need to reason about your service - an example being Service Bus queue, topic, and/or subscription names.
Like before, we’ll use the Visual Studio 
Add project
 wizard to create our new 
Contracts
 library.
Right-click the solution, select 
Add… → New Project
 and you will be presented as the same menu from before.
Find the 
Class Library
 template this time, and name it identically to your 
Consumer
 project - but use 
Contracts
 as the suffix instead.
IC.{Domain}.{Service/SubSystem}.Contracts
e.g. 
IC.Biller.ReportGenerator.Contracts
Since this does need to be published for other projects to consume & use, you will need to select the framework version based upon your own needs - note that some framework versions are incompatible with newer versions of C# making certain language features unavailable - you may need to adapt to this in subsequent examples if this ends up being your scenario.
 
.NET Framework (4.7.x <= n <= 4.8.x), select 
.NET Standard 2.0
 as the Framework. 
If any new projects consuming this library will be .NET 8.0 or greater, exclusively - simply use 
.NET 8.0
For the remaining examples - 
.NET Standard 2.0
 is used for the 
Contracts
 library.
Name the class Library
Select the appropriate framework version
C. Add Commands, Events, and Other Shared Items to 
Contracts
Commands and Events
Commands & Events are added on a case-by-case basis - these should be modeled beforehand using requirements gathered from stakeholders - again, 
Review Best Practices on Message - Commands + Events
.
note
.NET Standard 2.0 does not support the 
record
 type - but I still highly recommend defaulting to this if you are using a compatible .NET version. 


.NET Standard 2.0 does not support the 
record
 type - but I still highly recommend defaulting to this if you are using a compatible .NET version. 


Once you have determined which commands/events are needed in your own service, create folders for each within the contracts project - you should have a structure somewhat similar to this:
Folder Structure for Contracts Project with Example Message Types
Ensure that you have the following three properties in each command/event type:
Guid CorrelationId
The correlation Id of the command or event that allows tracing a request end-to-end. 
Batches of commands/events should all share the same 
CorrelationId
 property when they’re created and any subsequent commands or events that are produced in reaction to those commands/events should set their 
CorrelationId
 property from the originating message(s).
string Username
The user or process that initiated the command or event - if reacting to another command/event, it should be set from the original command/event.
DateTime Timestamp
 
When the command or event occurred - 
THIS SHOULD ALWAYS BE IN UTC 
DateTime.UtcNow
You 
may
 use inheritance if you don’t wish to continually add these properties to large numbers of message types - however, you will be unable to generically consume those messages downstream. Each 
IHandleMessage<T>
 instance should target the 
most specific 
type.
AssemblyInfo
One of the easiest ways for external consumers of our services commands & events to utilize them effectively is to expose a static 
Assembly
 reference from the project; this is a pretty simple step.
Create a 
Properties
 folder within the 
Contracts
 project.
Add a single 
AssemblyInfo.cs
 class file inside the 
Properties
 folder.
Add the following code - adapting the naming convention to your own needs
c#
namespace IC.Biller.ReportGenerator.Contracts.Properties
{
    using System.Reflection;

    public static class BillerReportGeneratorContracts
    {
        public static Assembly Value => typeof(BillerReportGeneratorContracts).Assembly;
    }
}
D. Wire up the Rebus SDK
Now that we’ve done most of the work to add our different message types & created the necessary infrastructure for sharing those messages out to external producers or consumers - we can return to our worker service we created back in step 1 and begin actually using the Rebus SDK.
E. Create Consumers
For each message type that we wish for our service to handle, we need to create an implementation of Rebus’s 
IHandleMessages<T>
 interface. These can exist in their own 
Consumers
 folder within the project root - but I would recommend eventually adding or nesting them into either a 
Common
 or 
Core
 CS project or folder within your project as it grows larger (or if the project is already large enough to have one).
Two things to note below, first the 
IHandleMessages<T>
 interface being inherited will take any of our messages we’ve configured in our 
Contracts
 project as a type argument and second, we are injecting the 
IBus
 interface into the constructor. Once Rebus has been registered in 
Program.cs
, the 
IBus
 interface will resolve to the 
RebusBus
 concrete instance and allow interop with the SDK for sending, publishing, receiving, or responding to messages.
c#
// class implementing `IHandleMessages<T>` where `T` is our message type.
public class BillerReportEnqueuedConsumer: IHandleMessages<BillerReportEnqueued>
{
    // Inject whatever is needed to handle the event - repository, logger, services, settings, etc.
    private readonly IBillerReportRepository _repository;
    private readonly IBus _messageBus;
    private readonly ILogger<BillerReportEnqueuedConsumer> _logger;
    public class BillerReportEnqueuedConsumer(IBillerReportRepository repository, IBus messageBus, ILogger<BillerReportEnqueuedConsumer> logger)
    {
        this._repository = repository;
        this._messageBus = messageBus;
        this._logger = logger;
    }
    
    public async Task Handle(BillerReportEnqueued @event)
    {
       // To Implement.
    }
}
Let’s look at a basic example of how a handler may be implemented
c#
    public async Task Handle(BillerReportEnqueued @event)
    {
        _logger.LogInformation("Received {billerreportenqueued} event for Biller {id} at {timestamp}", nameof(BillerReportEnqueued), @event.BillerID, DateTime.UtcNow);
        
        // Simply pass the event along
        var result = await _reportDispatcher.ExecuteAndDispatchReportAsync(@event);
        
        // Example of using the `IBus` to publish another event.
        await this._messageBus.Publish(new BillerReportProcessed(@event.CorrelationId, @event.Username, @event.BillerID, @event.ReportGeneratorQueueID, result.ReportLocation?.ToString()));
    }
In the above example, we
Receive a message from the Azure Service Bus input queue, directly.
Perform some work - in this pseudocode example above, we’re very simply just passing the event along, assuming it has the necessary properties for the service to determine what to do.
Use the injected 
IBus
 interface to publish an event in response to the work being done.
This is just an example; each message consumer is unique to the task at hand - you may choose to implement some additional logging or other performance monitoring as part of the consumer behavior. Consumers also do not always need to publish additional events in response to actions being taken. 
This is a good example for when you 
may
 want to emit an event as a reaction to work being completed in a consumer - if we emit 
BillerReportProcessed
, we may have a notification service downstream that can consume that event on its own and subsequently handle sending an email notification to users or completing additional work - such as building metrics on how many reports we’ve run, in real time. Again, each scenario is unique and very much based on the requirements of the task at hand.
F. Register Consumers
Now that we have a consumer created in our project, we need to tell Rebus that it exists & where to source messages from.
Over in our 
Program.cs
 file, we need to configure Rebus on our 
IServiceCollection
c#
services
   .AddSingleton<IMySingletonService, MySingletonService>()
   // Other registrations.
   .AddRebus(cfg =>    
      cfg
        .Logging(l => l.ColoredConsole(minLevel: Rebus.Logging.LogLevel.Debug)) // Optional logging config.
        .Options(o => o.UseKebabCaseTopicNames()) // This is a custom class internal to the project, see below.
        .Transport(t => 
           t.UseAzureServiceBus(connString, "service-input-queue-name", new ChainedTokenCredential([new VisualStudioCredential(), new ManagedIdentityCredential()]))
            .AutomaticallyRenewPeekLock()),
            onCreated: async bus => {
                await Task.WhenAll(
                        // Auto-creates the ASB Topic Subscription for the message type.
                        bus.Subscribe<BillerReportEnqueued>(),
                        
                        // TODO - Can add more event subscriptions here as needed.
                );
            }
)
// Registers the `IHandleMessages<BillerReportEnqueued>` implementation.
.AddRebusHandler<BillerReportEnqueuedConsumer>();



/**
 * NOTE - this is in the `IC.Messaging.Core` NuGet package.
 * https://dev.azure.com/invoicecloud/Src/_git/IC.Messaging.Core 
 */ 
// This extension method conventionalizes the topic names to kebab-case
// e.g. 'BillerReportGenerator' -> 'biller-report-generator'
public static partial class ShortAndReadableTopicsRebusConfigurationExtensions
{
    public static void UseKebabCaseTopicNames(this OptionsConfigurer configurer)
    {
        configurer.Decorate<ITopicNameConvention>(c => new KebabCaseTopicNamesConvention());
    }

    private partial class KebabCaseTopicNamesConvention : ITopicNameConvention
    {
        public string GetTopic(Type eventType)
        {
            var separator = '-';

            // Split on Capital Letters
            var regex = KebabCaseRegex();

            var result = regex.Replace(eventType.Name, m => separator + m.Value).ToLowerInvariant();

            return result;
        }
    }

    private static Regex KebabCaseRegex() => SplitPascalCaseToWordsRegex();

    [GeneratedRegex("(?<=[a-z0-9])[A-Z]", RegexOptions.Compiled)]
    private static partial Regex SplitPascalCaseToWordsRegex();
}

With this registration done, we have
Registered an Event Subscription with Rebus - this will auto-create the Service Bus topic for the message type and create a subscription on the topic for that message type. 
The Subscriptions will forward all messages from the topic to the service’s input queue (
service-input-queue-name
, above).
Registered a consumer via 
IHandleMessages<T>
 implementation. All messages of the specified type received on the service’s input queue will be handled by the specified consumer implementation.
Enabled Logging for the bus.
Registered the 
IBus
 interface to the 
RebusBus
 implementation, making it available to use with dependency injection.
G. Produce Messages
TODO: This isn’t quite complete yet - this is happy-path behavior but needs to be expanded upon using an example of an external service.
Now that we have an event consumer, we need other services to be able to send commands or publish messages over the Service Bus so that the consuming service or job can react to those events and perform some work.
We saw an example above of publishing a message from a consumer job, but more often than not we’ll be wanting to send commands or publish events from an external service for the consumer to consume.
The producer will need the same packages utilized by the consumer job - added in 
Step A
 - but registration is quite a bit simpler
.NET with IoC
c#
serviceCollection
  .AddRebus(cfg =>
    cfg
        .Options(o => o.UseKebabCaseTopicNames())
        .Transport(t => 
                t.UseAzureServiceBusAsOneWayClient(azureServiceBusConnectionString, new ChainedTokenCredential([new VisualStudioCredential(), new ManagedIdentityCredential()])))
);
TODO: .NET Without IOC - using static 
Configure.With(activator)
 approach
IMPORTANT
 
One thing to note above is that the transport registration is now using 
UseAzureServiceBusAsOneWayClient()
. This denotes that Rebus will be used 
only for sending or publishing messages by this service.
 If you have a service that requires both sending and receiving messages, use the same style of registration used by the Consumer, above.
Once Rebus has been registered in a producing service, all that’s left to do is add some code that handles the message publication.
c#
// A simple service that takes many report Ids for a single biller & emits events for each report.
public sealed class EnqueueBillerReportService : IEnqueueBillerReportService
{
    private readonly IBus _bus;
    private readonly ILogger<EnqueueBillerReportService> _logger;

    public EnqueueBillerReportService(IBus bus, ILogger<EnqueueBillerReportService> logger)
    {
        this._logger = logger;
        this._bus = bus;
    }

    public async Task EnqueueManyBillerReportsAsync(int billerId, IEnumerable<int> reportIds, string userName, ReportDestinations destination = ReportDestinations.EmailWithDownloadLink, ReportFormats formats = ReportFormats.ZIPCSV)
    {
        var correlationId = $"{Guid.NewGuid()}";
        var events = reportIds.Select(rpt => new BillerReportEnqueued(correlationId, userName, billerId, rpt, destination, formats));

        foreach (var @event in events)
        {
            await this._bus.Publish(@event);
            this._logger.LogInformation("Enqueued Biller Report: {ReportId} for Biller: {BillerId}", @event.ReportGeneratorQueueID, @event.BillerID);
        }
        
        // This could also be parallelized
        var tasks = events.Select(async @event => {
            await this._bus.Publish(@event);
            this._logger.LogInformation("Enqueued Biller Report: {ReportId} for Biller: {BillerId}", @event.ReportGeneratorQueueID, @event.BillerID);
            }).ToList();
            
        await Task.WhenAll(tasks);
    }
}
4. Deploy to Azure Container Registry
note
Note that the remaining sections in this guide cover the step-by-step process of manually building and deploying a Docker container to Kubernetes. This will be largely handled by the CI/CD pipelines - but these steps are very useful for gaining an understanding of working with containerized applications and getting them deployed into Kubernetes. The steps covered here will still be quite useful for testing and verifying that your job works and is able to be orchestrated properly before you deploy it.


Note that the remaining sections in this guide cover the step-by-step process of manually building and deploying a Docker container to Kubernetes. This will be largely handled by the CI/CD pipelines - but these steps are very useful for gaining an understanding of working with containerized applications and getting them deployed into Kubernetes. The steps covered here will still be quite useful for testing and verifying that your job works and is able to be orchestrated properly before you deploy it.


Returning to our Docker Setup section above
, we should be able to easily containerize the Consumer job and get it deployed now. If you have not already, follow those steps to produce a 
Dockerfile
 for your service.
Prerequisites
Azure CLI
Docker CLI
 or 
Podman
Follow the below steps to produce a Docker image, tag it, and push it up to Azure Container Registry.
Build the Docker Image
docker build -f path/to/project/DockerFile .
 or 
podman build -f path/to/project/DockerFile .
If the build is successful, you should have produced a Docker image that will be represented by a hashed value. We will need to tag this image to provide it with a name and version so it can be easily identified.
powershell
> docker image ls

REPOSITORY                                      TAG              IMAGE ID      CREATED       SIZE
<none>                                          <none>           6e1efa7cdd1e  4 hours ago   224 MB
<none>                                          <none>           eed42008781d  4 hours ago   1.62 GB
# Other images.
Tag the Docker Image
docker tag <HASH-ID> <acrName>.azurecr.io/<name-of-service>:<version-of-service>
  or
podman tag <HASH-ID> <acrName>.azurecr.io/<name-of-service>:<version-of-service>
 
e.g.
> docker tag 6e1efa7cdd1e acrarchpoc.azurecr.io/biller-report-generator:0.0.1-rc.1

> docker image ls 
REPOSITORY                                      TAG              IMAGE ID      CREATED       SIZE
acrarchpoc.azurecr.io/biller-report-generator   0.0.1-rc.1       6e1efa7cdd1e  4 hours ago   224 MB
Login to Azure and ACR using the 
az
 
CLI
See the documentation on this process for more information on troubleshooting
powershell
> az login # This will open a browser window where you will verify your SSO login.

> az acr login -n <acr-name> --expose-token

{
  "accessToken": <JWT string>,
  "loginServer": "<acr-name>.azurecr.io"
}

> docker login <acr-name>.azurecr.io --username 00000000-0000-0000-0000-000000000000 --password "accessToken-fromabove"



# Recommended best practice is to store the access token
> TOKEN=$(az acr login --name <acrName> --expose-token --output tsv --query accessToken)

# Then login, passing the token via standard input
> docker login myregistry.azurecr.io --username 00000000-0000-0000-0000-000000000000 --password-stdin <<< $TOKEN
Push the container image to the Azure Container Registry Instance
powershell
> docker push <fully-clarified-image-tag-name>

# e.g.
> docker push acrarchpoc.azurecr.io/biller-report-generator:0.0.1-rc.1
5. Deploy to Kubernetes using Helm
Once the container has been successfully built and pushed to Azure Container Registry, it can be applied to a target Kubernetes cluster or a local 
minikube
 instance.
Prerequisites
Azure CLI
kubectl CLI
 → Use 
az aks install-cli
 for this.
Helm
Kubernetes Config
In order to interact with our target k8s cluster, we need to first authenticate - this can be done using the 
az
 CLI.
az aks get-credentials --resource-group myResourceGroup --name myAKSCluster
Running this command should add login credentials to a file within your user home directory:
C:\<User>\.kube\config
From this point forward - you can use 
kubectl
 to drive cluster interaction. Going through all use-cases of the 
kubectl
 CLI is beyond the scope of this documentation, but here’s a quick cheat sheet to get you started with navigating between clusters/namespaces and interacting with objects & pods.
powershell
## Get the Current Context
> kubectl config current-context

## Change Current Context
> kubectl config use-context <aks-cluster-name>

## Change Namespace in Current Context
> kubectl config set-context --current --namespace=<my-namespace>

## Create New Context
kubectl set-context <name>

## Get Items
> kubectl get pods|namespace|deployments|services|ingresses

## Log in to a Pod
kubectl exec -it <k8s-pod-name> -- /bin/sh # Could also be /bin/bash
Deployment
This process is short-circuited quite a bit by the presence of a Helm chart specifically for these types of scalable workloads. You may find the 
keda-scaled-object
 Helm chart in the Platform project, 
here
.
To test the Helm chart, we will need a yaml file containing a collection of settings for the deployment to use - these will include application config settings, scaling rules, pod resource rules, etc.
Since we already know the chart we need - there is an easy path to get the required yaml - it is simply the 
values.yaml
 file included in the 
charts/keda-scaled-object
 Chart linked above.
Default Values File - Local to the Helm Chart
yaml
replicaCount: 1

nodeSelector:
  agentpool: job

image:
  repository: "" 
  pullPolicy: Always
  tag: ""

appName: "my-app"

clientId: "a90feb07-dfd9-4940-8ae3-dc637e2f75db"
tenantId: "6ee1d96e-4c90-42a8-be8a-f94078515d91"

instance: "primary"
environment: "dev"

# Adjust as needed.
resources:
  limits:
    memory: 512Mi
  requests:
    cpu: 512Mi
    memory: 512Mi

# HPA Config - used for KEDA
# https://keda.sh/docs/2.17/reference/scaledobject-spec/
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 25
  pollingInterval: 30
  cooldownPeriod: 300
  initialCooldownPeriod: 0

# KEDA ServiceBus Scaler Config
# https://keda.sh/docs/2.17/scalers/azure-service-bus/
serviceBus:
  # This is the Azure Service Bus namespace
  namespace: ""
  # The entity on which to trigger auto-scaling.
  queueName: ""
  messageCount: '"10"'
  activationMessageCount: '"10"'

# Worker service TCP Port for Health Checks.
healthCheck: null

# Liveness & Readiness Probes 
livenessProbe: null

readinessProbe: null


# The name of the KeyVault to pull Secrets from at runtime using application sdk
keyVaultName: ""

secretProvider: null

extraEnvVars: null

# Setting this to true will inject OTEL specific Environment Variables into your Container for use with the OTEL SDK.
enableOpenTelemetry: false
Environment Specific File - Local to the Service (Consumer) Repository
yaml
appName: biller-report-generator

nodeSelector:
  agentpool: userpool

image:
  repository: acrarchpoc.azurecr.io/biller-report-generator
  pullPolicy: Always
  tag: "0.0.1-rc.2"

extraEnvVars:
  - name: ASPNETCORE_ENVIRONMENT
    value: "Development"
  - name: IC__ServiceDefaults__HealthCheckTcpPort
    value: 5000
  - name: IC__ServiceDefaults__InvoiceCloudSupportEmail
    value: "yourEmail@invoicecloud.com"
  - name: IC__ServiceDefaults__EmailEnabled
    value: "true"
  - name: IC__ServiceDefaults__EmailRetryCount
    value: 2
  - name: IC__ServiceDefaults__StorageContainer
    value: "ic-reporting"
  - name: IC__ServiceDefaults__RebusInputQueue
    value: "biller-report-generator"
  - name: IC__MailgunSmtpCredentials__Port
    value: 587 
  - name: IC__AzureStorage__SASExpirationInHours
    value: 24 
 
keyVaultName: arch-kv-poc

healthCheck:
  port: 5000

livenessProbe:
  tcpSocket:
    port: health-check
  initialDelaySeconds: 10
  timeoutSeconds: 30
  periodSeconds: 60
  successThreshold: 1
  failureThreshold: 5

readinessProbe:
  tcpSocket:
    port: health-check
  initialDelaySeconds: 10
  timeoutSeconds: 5
  periodSeconds: 15
  successThreshold: 1
  failureThreshold: 3


secretProvider:
 keyVaultName: arch-kv-poc
 secretObjects: 
   - secretName: MailGunUsername
     envVarName: IC__MailGunSmtpCredentials__Username
   - secretName: MailGunPassword
     envVarName: IC__MailGunSmtpCredentials__Password
   - secretName: AzureSqlConnectionString
     envVarName: IC__ConnectionStrings__AzureSql
   - secretName: AzureServiceBusConnectionString
     envVarName: IC__ConnectionStrings__AzureServiceBus
   - secretName: AzureStorageUri
     envVarName: IC__AzureStorage__AccountUri

serviceBus:
  namespace: srvbus-arch
  queueName: biller-report-generator
  messageCount: '"10"'
  activationMessageCount: '"10"'

enableOpenTelemetry: true
Using these settings in the Helm chart will create the containerized consumer workload and all dependent infrastructure.
Finally, apply the chart to the cluster using 
Helm
powershell
> helm install <chart-name> --create-namespace --namespace <service-name> -f values.yaml -f env-values.yaml .
Note above that 
--namespace
 will align with the application name - 
there should be 1 namespace per service, this will contain all of the dependent infrastructure for the job.
You can pass as many 
yaml
 files as you wish to the command, note that they’ll be applied from left to right, with the rightmost settings taking precedence and overriding any values that have been passed by preceding 
yaml
.
Once the chart has been deployed - you should see your resources begin appearing in k8s - you can login to the Azure Portal to verify or use 
kubectl get <item>
 to verify from the CLI.
6. Scaling with KEDA
Kubernetes Event-Driven Autoscaling
 is used to provide some glue between 
Kubernetes Horizontal Pod Autoscaling
 and external services - KEDA makes it simple to trigger automatically scale workloads based on message queue volume, database records, etc.
This guide is assuming that KEDA itself will already have been configured in the k8s cluster by the time you are ready to deploy.
There are two types of objects that KEDA provides, 
ScaledObject
 and 
ScaledJob
. We’re focusing specifically on the 
ScaledObject
 definition and the 
Azure Service Bus scaler
.
The full definition of the scaler can be viewed in the last link above - the values relevant to our Helm deployment can be seen in the 
yaml
 files above.
yaml
serviceBus:
  namespace: srvbus-arch # The Azure Service Bus Namespace
  queueName: biller-report-generator # The Service Bus queue for the service - should align 1:1 with name of app/service.
  messageCount: '"10"' # Number of messages in queue at which KEDA begins scaling the workload.
  activationMessageCount: '"10"' # Number of messages in queue before KEDA activates the scaler.
At present - we are only concerned with scaling on the 
queueName
 - this will be 1:1 with the 
service-input-queue-name
 that we used when 
registering the consumers, above
. It is also possible to use a ServiceBus 
topicName
 and 
subscriptionName
 to trigger KEDA scaling as well - this may be desired behavior in the future.
Read a little on KEDA’s 
2-phase scaling process
 - the values set in these fields drive this behavior.
activationMessageCount
 - This drives the activation of the KEDA scaler. Essentially, when we hit this message threshold KEDA will activate the scaler, so it begins proactively watching the queue to hit the 
messageCount
 
messageCount
 - The number of messages to begin auto-scaling the workload.
You can test how the scaler will behave under load by introducing a volume of messages into the target SB queue (in a local or development environment, please) and watching Kubernetes react by creating additional pods under the workload deployment.
Related Resources
https://invoicecloud.atlassian.net/wiki/spaces/EA/pages/edit-v2/4112351261
 
Messaging - Commands and Events
 
https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions
 
Rebus Documentation
 
Azure CLI
Docker CLI
 
Podman
Introduction to kubectl | Kubernetes
https://helm.sh/
 
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
 
Getting Started | KEDA
 URL:/spaces/PMK/pages/4279304221/QuickStart+-+.Net+Core+Event+Driven+Job+k8s+Containerized