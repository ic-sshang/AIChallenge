none
Timeframe
5/29/2024 9:30 AM - 11 AM Eastern Time
Load Testing Scenarios
We executed one Load Testing run of ~30 minute.
Scenarios and Endpoints Exercised
CRM
Land on Login Page and Login
devtest{X}.invoicecloud.com/crm/AdminLogin.aspx
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/count
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/unassigned
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/pastdue
Navigate to Biller
devtest{X}.invoicecloud.com/crm/webservices/CRMSearch.asmx/GET_CRM_SearchResults
devtest{X}.invoicecloud.com/crm/AdminHome.aspx
devtest{X}.invoicecloud.com/crm/Blank.aspx
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/unassigned
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/pastdue
devtest{X}.invoicecloud.com/crm/BillerProducts.aspx?bid=2002&bg=4851516b-28fa-4194-8b4a-7097ff11c4b8
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/count
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/unassigned
devtest{X}.invoicecloud.com/crm/billertracksteps/employee/368/pastdue
Logout
devtest{X}.invoicecloud.com/crm/BillerProducts.aspx?bid=2002&bg=4851516b-28fa-4194-8b4a-7097ff11c4b8
Biller Portal
Land and Login
devtest{X}.invoicecloud.com/portal/(S(mdn0lzjjilvp0u2pvhz3gg5b))/billerlogin.aspx
Navigate to Customer Search Page
devtest{X}.invoicecloud.com/portal/(S(y1d0jj1uglfyl15kazudg1ue))/BillerCustomers.aspx
Search for Customer
devtest{X}.invoicecloud.com/portal/(S(y1d0jj1uglfyl15kazudg1ue))/BillerCustomers.aspx
Logout
devtest{X}.invoicecloud.com/portal/(S(y1d0jj1uglfyl15kazudg1ue))/BillerCustomers.aspx
Customer Portal
Land and Login
devtest{X}.invoicecloud.com/portal/(S(wkn3tnx3b5p0g4bh2peuercg))/2/CustomerLogin.aspx?BillerGUID=4851516b-28fa-4194-8b4a-7097ff11c4b8&iti=
View Invoice
devtest{X}.invoicecloud.com/portal/(S(wkn3tnx3b5p0g4bh2peuercg))/2/customergroupledger.aspx?mode=open
Log Out
devtest{X}.invoicecloud.com/portal/(S(wkn3tnx3b5p0g4bh2peuercg))/2/customergroupledger.aspx?mode=open
Kong
kong.dev.invoicecloud.com/api
kong.dev.invoicecloud.com/
Payer NextGen
api.{env}.invoicecloud.com/payer/v1/one-time-payment-api/health
api.{env}.invoicecloud.com/payer/v1/payer-portal-ui/health
api.{env}.invoicecloud.com/payer/v1/pay-by-text-api/health
Node Configuration
Namespace                   Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                           ------------  ----------  ---------------  -------------  ---
  kong                        api-gateway-kong-5f4bb66f65-gmhrw                              900m (47%)    0 (0%)      800Mi (14%)      0 (0%)         13d
  kube-system                 azure-ip-masq-agent-m4qpl                                      100m (5%)     500m (26%)  50Mi (0%)        250Mi (4%)     13d
  kube-system                 cloud-node-manager-cfb5g                                       50m (2%)      0 (0%)      50Mi (0%)        512Mi (9%)     6d9h
  kube-system                 csi-azuredisk-node-zgdd5                                       30m (1%)      0 (0%)      60Mi (1%)        400Mi (7%)     6d9h
  kube-system                 csi-azurefile-node-xjnb8                                       30m (1%)      0 (0%)      60Mi (1%)        600Mi (11%)    6d9h
  kube-system                 kube-proxy-h9knz                                               100m (5%)     0 (0%)      0 (0%)           0 (0%)         6d9h
  kube-system                 metrics-server-6bc6599b58-8qgps                                52m (2%)      147m (7%)   101Mi (1%)       371Mi (6%)     124m
  newrelic                    newrelic-bundle-newrelic-logging-k526z                         150m (7%)     300m (15%)  64Mi (1%)        128Mi (2%)     13d
  newrelic                    newrelic-bundle-nrk8s-kubelet-mskfm                            200m (10%)    0 (0%)      300M (5%)        600M (10%)     13d
  opentelemetry               opentelemetry-daemonset-opentelemetry-collector-agent-pd6st    100m (5%)     100m (5%)   200M (3%)        200M (3%)      13d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests          Limits
  --------           --------          ------
  cpu                1712m (90%)       1047m (55%)
  memory             1742562560 (31%)  3096514Ki (56%)
  ephemeral-storage  0 (0%)            0 (0%)
  hugepages-1Gi      0 (0%)            0 (0%)
  hugepages-2Mi      0 (0%)            0 (0%)
HPA Configuration
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
Results
Payer and Kong APIs
checks.........................: 99.92% ✓ 3233715    ✗ 2435
data_received..................: 424 MB 280 kB/s
data_sent......................: 36 MB  23 kB/s
group_duration.................: avg=1.02s    min=1s     med=1s     max=21.71s   p(90)=1.02s   p(95)=1.02s
http_req_blocked...............: avg=199.3µs  min=0s     med=0s     max=1.56s    p(90)=0s      p(95)=0s
http_req_connecting............: avg=99.96µs  min=0s     med=0s     max=72.45ms  p(90)=0s      p(95)=0s
http_req_duration..............: avg=27.72ms  min=0s     med=6.33ms max=21.63s   p(90)=16.78ms p(95)=24.47ms
  { expected_response:true }...: avg=10.35ms  min=0s     med=6.12ms max=4.55s    p(90)=16.34ms p(95)=23.71ms
http_req_failed................: 6.00%  ✓ 48615      ✗ 760594
http_req_receiving.............: avg=133.77µs min=0s     med=0s     max=139.39ms p(90)=514.9µs p(95)=639.1µs
http_req_sending...............: avg=36.24µs  min=0s     med=0s     max=1.08s    p(90)=0s      p(95)=228.65µs
http_req_tls_handshaking.......: avg=95.98µs  min=0s     med=0s     max=1.55s    p(90)=0s      p(95)=0s
http_req_waiting...............: avg=27.55ms  min=0s     med=6.2ms  max=21.63s   p(90)=16.53ms p(95)=24.12ms
http_reqs......................: 809209 533.477445/s
iteration_duration.............: avg=17.35s   min=17.07s med=17.19s max=37.98s   p(90)=17.28s  p(95)=17.34s
iterations.....................: 48037  31.668773/s
vus............................: 1      min=1        max=1200
vus_max........................: 1200   min=1200     max=1200

running (25m16.9s), 0000/1200 VUs, 48037 complete and 2 interrupted iterations
average_load ✓ [======================================] 0000/1200 VUs  25m0s
CRM Scenario
Biller Portal Scenario
Customer Portal Scenario
Metrics from NewRelic
Kong NonProd Traffic Graphs
https://onenr.io/0LwGPrYmAR6
Kong NonProd Cluster Summary
https://onenr.io/07j9PrxneRO
Kong NonProd Cluster Resource Graphs
https://onenr.io/08jqPqGoGRl
Kong NonProd Cluster Error Logs
https://onenr.io/08jqPqNOORl
DevTest VM Graphs
Analysis
The traffic load this run was similar to the previous Load Test, but this time during the initial ramp up we had a Kong Pod scale out event and a fourth Kong Pod took Load for a period of the test timeline, therefore the reduction in the HPA Average CPU Utilization from 70% to 60% was a successful improvement in the HPA Configuration.
Average Node CPU Utilization for Kong Nodes did not exceed 60% which is well within an acceptable saturation range.  Anything above 80% would be concerning.
Average Node Memory Utilization for Kong Nodes did not exceed 70% which is within an acceptable saturation range.  Anything above 80% would be concerning.
Error Rate reported by each of the Test Platform/Use Cases were all around ~7%, which is under the 10% Error Rate which would be the rate of Error that would begin to become concerning for Load Tests inside a poorly configured set of Backend Web Servers, a single Proxy Server, and a lower SKU SQL Server, also considering all traffic was related to a single Biller which would not be the case in Production.  Additionally, the Errors Logged by Kong were overwhelmingly based on Upstream Server timeouts.
The most saturated Processing Plane was the single Proxy Server which ushers SQL DB requests and Storage Account requests.  It reached maximum saturation of 80% CPU Utilization.
My recommendation is to promote this configuration to Production with a minimum of 3 Nodes/Pods with the ability to scale out to 10 Nodes/Pods, then monitor the metrics in NewRelic closely for a week and up the minimum Node/Pods if it is observed that spikes in traffic occur and cause saturation levels of over 75% Node CPU utilization at any given time.
 URL:/spaces/platform/pages/3275391001/Kong+DV2+-+Round+3+Load+Testing